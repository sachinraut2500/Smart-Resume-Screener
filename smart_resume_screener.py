# -*- coding: utf-8 -*-
"""Smart Resume Screener

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# =========================
# requirements.txt
# =========================
"""
spacy>=3.5.0
sentence-transformers>=2.2.2
pandas>=1.5.0
scikit-learn>=1.1.0
tqdm>=4.64.0
regex
"""

# =========================
# src/__init__.py
# =========================
# (empty file to mark src as a package)

# =========================
# src/preprocess.py
# =========================
import os
import json
import argparse
from pathlib import Path
from typing import List
import pandas as pd
import re
import spacy

COMMON_SKILLS = {
    "python","java","c++","c#","sql","nosql","docker","kubernetes",
    "aws","azure","gcp","pytorch","tensorflow","scikit-learn",
    "pandas","numpy","spark","hadoop","nlp","computer vision",
    "react","angular","node.js","django","flask","git","linux"
}

def clean_text(t: str) -> str:
    t = t.replace("\r", " ")
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def extract_skills_spacy(text: str, nlp) -> List[str]:
    doc = nlp(text.lower())
    found = set()
    for skill in COMMON_SKILLS:
        if skill in text.lower():
            found.add(skill)
    for chunk in doc.noun_chunks:
        tok = chunk.text.strip()
        if len(tok) <= 40 and len(tok.split()) <= 4:
            if any(w in tok for w in ["machine","learning","deep","neural","computer","vision","natural","language","processing","data","engineer","developer","framework"]):
                found.add(tok)
    processed = {re.sub(r"\s{2,}", " ", s.lower()).strip() for s in found}
    return sorted(processed)

def load_text_file(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")

def process_directory(input_dir: Path, out_csv: Path, spacy_model: str = "en_core_web_sm"):
    print(f"Loading spaCy model '{spacy_model}'...")
    nlp = spacy.load(spacy_model, disable=["parser"])
    rows = []
    for p in sorted(input_dir.glob("*.txt")):
        text = clean_text(load_text_file(p))
        skills = extract_skills_spacy(text, nlp)
        rows.append({"file": p.name, "text": text, "skills": json.dumps(skills)})
        print(f"{p.name}: {len(skills)} skills")
    pd.DataFrame(rows).to_csv(out_csv, index=False)
    print(f"Wrote {out_csv}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Input dir with .txt files")
    parser.add_argument("--out", default="data/processed.csv", help="Output CSV")
    parser.add_argument("--spacy-model", default="en_core_web_sm")
    args = parser.parse_args()
    process_directory(Path(args.input), Path(args.out), args.spacy_model)

# =========================
# src/embedding_utils.py
# =========================
from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from pathlib import Path
from typing import List, Dict

MODEL_NAME = "all-MiniLM-L6-v2"

class Embedder:
    def __init__(self, model_name: str = MODEL_NAME):
        print(f"Loading embedding model '{model_name}' ...")
        self.model = SentenceTransformer(model_name)

    def encode(self, texts: List[str], batch_size: int = 32, show_progress_bar: bool = False) -> np.ndarray:
        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=show_progress_bar, convert_to_numpy=True)

def cache_embeddings(path: Path, items: Dict[str, np.ndarray]):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "wb") as f:
        pickle.dump(items, f)

def load_cached_embeddings(path: Path):
    if not path.exists():
        return {}
    with open(path, "rb") as f:
        return pickle.load(f)

# =========================
# src/matcher.py
# =========================
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
from tqdm import tqdm
from embedding_utils import Embedder, cache_embeddings, load_cached_embeddings

CACHE_DIR = Path("data/.emb_cache")

def read_processed_csv(path: Path):
    df = pd.read_csv(path)
    if "text" not in df.columns:
        raise ValueError("CSV missing 'text' column")
    return df

def compute_embeddings(items: pd.Series, embedder: Embedder, cache_file: Path):
    cache = load_cached_embeddings(cache_file)
    texts = items.tolist()
    embeddings = []
    to_compute, indices = [], []
    for i, t in enumerate(texts):
        key = str(hash(t))
        if key in cache:
            embeddings.append(cache[key])
        else:
            embeddings.append(None)
            to_compute.append(t); indices.append(i)
    if to_compute:
        new_emb = embedder.encode(to_compute, show_progress_bar=True)
        for j, idx in enumerate(indices):
            key = str(hash(texts[idx]))
            embeddings[idx] = new_emb[j]
            cache[key] = new_emb[j]
        cache_embeddings(cache_file, cache)
    return np.vstack(embeddings)

def top_k_similarities(job_vec, resume_vecs, k=5):
    sims = cosine_similarity(job_vec.reshape(1, -1), resume_vecs).reshape(-1)
    top_idx = np.argsort(-sims)[:k]
    return list(zip(top_idx.tolist(), sims[top_idx].tolist()))

def skills_overlap(resume_skills_json: str, job_text: str):
    try:
        resume_skills = json.loads(resume_skills_json)
    except Exception: resume_skills = []
    job_lower = job_text.lower()
    return [s for s in resume_skills if s and s.lower() in job_lower]

def run_matching(resumes_csv: Path, jobs_dir: Path, out_csv: Path, top_k: int = 5):
    df = read_processed_csv(resumes_csv).reset_index().rename(columns={"index": "resume_idx"})
    jobs = [{"job_file": p.name, "text": p.read_text(encoding="utf-8", errors="ignore")} for p in sorted(jobs_dir.glob("*.txt"))]
    embedder = Embedder()
    resume_vecs = compute_embeddings(df["text"], embedder, CACHE_DIR / "resumes.pkl")
    results = []
    for job in tqdm(jobs, desc="Jobs"):
        job_vec = embedder.encode([job["text"]])[0]
        top_matches = top_k_similarities(job_vec, resume_vecs, k=top_k)
        for idx, score in top_matches:
            res = df.iloc[idx]
            results.append({
                "job": job["job_file"],
                "resume": res["file"],
                "similarity": round(score, 4),
                "skills_overlap": ", ".join(skills_overlap(res["skills"], job["text"]))
            })
    pd.DataFrame(results).to_csv(out_csv, index=False)
    print(f"Wrote matches to {out_csv}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--resumes", required=True, help="CSV from preprocess.py")
    parser.add_argument("--jobs", required=True, help="Dir with job description .txt")
    parser.add_argument("--out", default="results.csv")
    parser.add_argument("--topk", type=int, default=5)
    args = parser.parse_args()
    run_matching(Path(args.resumes), Path(args.jobs), Path(args.out), args.topk)